---
title: "Chapter 2: Multi-Armed Bandits"
number-sections: true
---

## 2.1 The $k$-armed Bandit Problem {#sec-ch02-2-1}

- The particular nonassociative, evaluative feedback problem is a simple version of the $k$-armed bandit problem.

$$q_*(a) \doteq \mathbb{E}\left[R_t \mid a_t = a\right] \quad \text{action-value function}$$

- $Q_t(a)$: estimated action-value of action $a$ at time $t$

$$Q_t(a) \to q_*(a)$$

- **Exploitation**: always selecting the greedy actions (actions with highest estimated value)
- **Exploration**: selecting one of the nongreedy actions
- There exists an **Exploration-Exploitation tradeoff**

---

## 2.2 Action-value Methods {#sec-ch02-2-2}

$$Q_t(a) \doteq \frac{\text{sum of rewards when } a \text{ taken prior to } t}{\text{number of times } a \text{ taken prior to } t} = \frac{\sum_{i=1}^{t-1} R_i \cdot \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}}$$

where $a$ = action, $t$ = timestep.

Where $\mathbb{1}_{\text{predicate}} \equiv$ random variable that is $1$ if predicate is true & $0$ if it's not.

As $\displaystyle\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a} \to \infty$, by the law of large numbers, $Q_t(a)$ converges to $q_*(a)$.

- Simplest action selection rule: **greedy method** (no exploration)

$$A_t \doteq \arg\max_a Q_t(a)$$

- **$\varepsilon$-greedy method** $\Rightarrow$ <br>simplest idea for ensuring continual exploration. With $m$ total actions:
  - With probability $1-\varepsilon$: choose the greedy action
  - With probability $\varepsilon$: choose a random action

$$\pi(a|s) = \begin{cases} \varepsilon/m + 1 - \varepsilon & \text{if } a^* = \arg\max_{a \in A} Q(s,a) \\ \varepsilon/m & \text{otherwise} \end{cases}$$

---

## 2.3 The 10-armed Testbed {#sec-ch02-2-3}

- Stationarity & determinism vs. nonstationarity & stochasticity
- Compare the relative effectiveness of greedy & $\varepsilon$-greedy action-value methods

---

## 2.4 Incremental Implementation {#sec-ch02-2-4}

$$Q_n \doteq \frac{R_1 + R_2 + \ldots + R_{n-1}}{n-1}$$

$$Q_{n+1} = \frac{1}{n} \sum_{i=1}^{n} R_i$$

$$= \frac{1}{n}\left(R_n + \sum_{i=1}^{n-1} R_i\right)$$

$$= \frac{1}{n}\left(R_n + (n-1)\frac{1}{n-1}\sum_{i=1}^{n-1} R_i\right)$$

$$= \frac{1}{n}\left(R_n + (n-1)Q_n\right)$$

$$= \frac{1}{n}\left(R_n + nQ_n - Q_n\right)$$

$$\boxed{Q_{n+1} = Q_n + \frac{1}{n}\left[R_n - Q_n\right]}$$

In general:

$$\text{NewEstimate} \leftarrow \text{OldEstimate} + \text{StepSize}\underbrace{\left[\text{Target} - \text{OldEstimate}\right]}_{\text{error}}$$

---

## 2.5 Tracking a Nonstationary Problem {#sec-ch02-2-5}

$$Q_{n+1} \doteq Q_n + \alpha\left[R_n - Q_n\right] \quad \alpha \in (0,1] \text{ is constant}$$

Expanding the recurrence:

$$Q_{n+1} = Q_n + \alpha[R_n - Q_n]$$

$$= \alpha R_n + (1-\alpha) Q_n$$

$$= \alpha R_n + (1-\alpha)\left[\alpha R_{n-1} + (1-\alpha) Q_{n-1}\right]$$

$$= \alpha R_n + (1-\alpha)\alpha R_{n-1} + (1-\alpha)^2 Q_{n-1}$$

$$= \alpha R_n + (1-\alpha)\alpha R_{n-1} + (1-\alpha)^2\left[\alpha R_{n-2} + (1-\alpha) Q_{n-2}\right]$$

$$= \alpha R_n + (1-\alpha)\alpha R_{n-1} + (1-\alpha)^2 \alpha R_{n-2} + (1-\alpha)^3 Q_{n-2}$$

$$= \alpha R_n + (1-\alpha)\alpha R_{n-1} + (1-\alpha)^2 \alpha R_{n-2} + (1-\alpha)^3 \alpha R_{n-3} + \ldots + (1-\alpha)^{n-1}\alpha R_1 + (1-\alpha)^n Q_1$$

$$\vdots$$

$$\boxed{Q_{n+1} = (1-\alpha)^n Q_1 + \sum_{i=1}^{n} \alpha(1-\alpha)^{n-i} R_i}$$

This is a **weighted average** (or **exponential recency-weighted average**): gives priority/higher value to more recently acquired rewards.

Since:

$$\left(1-\alpha\right)^n + \sum_{i=1}^{n} \alpha(1-\alpha)^{n-i} = 1$$

If $(1-\alpha) = 0 \Rightarrow \alpha = 1$, then:

$$Q_{n+1} = (1-\alpha)^n Q_1 + \sum_{i=1}^{n} \alpha(1-\alpha)^{n-i} R_i = 0^n Q_1 + \sum_{i=1}^{n} 1 \cdot (0)^{n-i} R_i = Q_1 + \sum_{i=1}^{n} R_i$$

$$Q_{n+1} = Q_1 + R_1 + R_2 + R_3 + \ldots + R_n$$

### Step-size Parameter

- $\alpha_n(a) = \frac{1}{n}$ (sample-average method) $\Rightarrow$ **unbiased**

Stochastic approximation theory (Robbins-Monro sequence) requires:

$$\text{(I)}\ \sum_{n=1}^{\infty} \alpha_n(a) = \infty \quad \text{[overcome random fluctuations]}$$

$$\text{(II)}\ \sum_{n=1}^{\infty} \alpha_n^2(a) < \infty \quad \text{[convergence]}$$

- (I) & (II) are met for $\alpha_n(a) = \frac{1}{n}$ (average method)
- (II) is **not** met for $\alpha_n(a) = \alpha$ (constant method) $\Rightarrow$ biased

---

## 2.6 Optimistic Initial Values {#sec-ch02-2-6}

- Compare greedy vs. $\varepsilon$-greedy method on $k$-armed testbed
- Technique for encouraging exploration is called **optimistic initial values** (focus on initial conditions)
- Effective on stationary problems, but not a general approach
- Not good for nonstationary problems

---

## 2.7 Upper-Confidence-Bound (UCB) Action Selection {#sec-ch02-2-7}

$$A_t \doteq \arg\max_a \left[Q_t(a) + c\sqrt{\frac{\ln(t)}{N_t(a)}}\right]$$

where:
- $N_t(a) \equiv$ number of times that action $a$ has been selected prior to time $t$
- $c > 0$ = controls the degree of exploration ($c \equiv$ confidence level)

If $N_t(a) = 0$, then $a$ is considered to be a maximizing action.

$\sqrt{\dfrac{\ln(t)}{N_t(a)}} \equiv$ measure of the uncertainty/variance in the estimate of $a$'s value.

- More difficult than $\varepsilon$-greedy to extend beyond bandits to the more general RL settings
- Difficulty in dealing with nonstationary problems
- Difficulty in dealing with large state spaces (function approximation)
- Introduces an exploration bonus term $c\sqrt{\ln(t)/N_t(a)}$

---

## 2.8 Gradient Bandit Algorithms {#sec-ch02-2-8}

$H_t(a) \in \mathbb{R} \Rightarrow$ numerical preference for each action $a$

$$\mathbb{P}\{A_t = a\} \doteq \frac{e^{H_t(a)}}{\sum_{b=1}^{k} e^{H_t(b)}} \doteq \pi_t(a) \quad \text{(soft-max distribution)}$$

Action preference update via stochastic gradient ascent:

$$H_{t+1}(A_t) \doteq H_t(A_t) + \alpha(R_t - \bar{R}_t)(1 - \pi_t(A_t))$$

$$H_{t+1}(a) \doteq H_t(a) - \alpha(R_t - \bar{R}_t)\pi_t(a) \quad \text{for all } a \neq A_t$$

where:
- $\alpha > 0 \Rightarrow$ step-size parameter
- $\bar{R}_t \in \mathbb{R} \Rightarrow$ average of the rewards up to but not including time $t$ ($\bar{R}_1 \doteq R_1$)
- $\bar{R}_t$ serves as a baseline with which the reward is compared

If $R_t > \bar{R}_t \Rightarrow \mathbb{P}(A_t = a)\uparrow$

If $R_t < \bar{R}_t \Rightarrow \mathbb{P}(A_t = a)\downarrow$

- The larger the preference ($H_t(a)$), the more often the action is taken.

### 2.8 Proof: The Bandit Gradient Algorithm as Stochastic Gradient Ascent {#sec-ch02-2-8}

Gradient ascent update:

$$H_{t+1}(a) \doteq H_t(a) + \alpha \frac{\partial \mathbb{E}[R_t]}{\partial H_t(a)}$$

where $\mathbb{E}[R_t] = \sum_x \pi_t(x) q_*(x)$.

Exact performance gradient:

$$\frac{\partial \mathbb{E}[R_t]}{\partial H_t(a)} = \frac{\partial}{\partial H_t(a)}\left[\sum_x \pi_t(x) q_*(x)\right] = \sum_x q_*(x) \frac{\partial \pi_t(x)}{\partial H_t(a)}$$

$$= \sum_x \left(q_*(x) - B_t\right) \frac{\partial \pi_t(x)}{\partial H_t(a)} \quad B_t = \text{baseline}$$

$$= \sum_x \pi_t(x)\left[q_*(x) - B_t\right] \frac{\partial \pi_t(x)}{\partial H_t(a)} \frac{1}{\pi_t(x)}$$

$$= \mathbb{E}\left[\left(q_*(A_t) - B_t\right) \frac{\partial \pi_t(A_t)}{\partial H_t(a)} \frac{1}{\pi_t(A_t)}\right] \quad \text{with } B_t = \bar{R}_t$$

$$= \mathbb{E}\left[\left(R_t - \bar{R}_t\right) \frac{\partial \pi_t(A_t)}{\partial H_t(a)} \frac{1}{\pi_t(A_t)}\right]$$

since $q_*(A_t) = R_t$ (because $\mathbb{E}[R_t|A_t] = q_*(A_t)$).

**Aside:**

$$\frac{\partial \pi_t(x)}{\partial H_t(a)} = \pi_t(x)\left[\mathbb{1}_{a=x} - \pi_t(a)\right]$$

where $\mathbb{1}_{a=x} = \begin{cases} 1 & \text{if } a = x \\ 0 & \text{otherwise} \end{cases}$

Therefore:

$$= \mathbb{E}\left[(R_t - \bar{R}_t)\,\pi_t(A_t)\left(\mathbb{1}_{a=A_t} - \pi_t(a)\right)\frac{1}{\pi_t(A_t)}\right]$$

$$= \mathbb{E}\left[(R_t - \bar{R}_t)\left(\mathbb{1}_{a=A_t} - \pi_t(a)\right)\right]$$

$$\Rightarrow \boxed{H_{t+1}(a) = H_t(a) + \alpha(R_t - \bar{R}_t)\left(\mathbb{1}_{a=A_t} - \pi_t(a)\right)} \quad \text{for all } a$$

**Proof that** $\dfrac{\partial \pi_t(x)}{\partial H_t(a)} = \pi_t(x)\left[\mathbb{1}_{a=x} - \pi_t(a)\right]$:

Using the quotient rule $\dfrac{\partial}{\partial z}\left[\dfrac{f(z)}{g(z)}\right] = \dfrac{f'(z)g(z) - f(z)g'(z)}{g(z)^2}$:

$$\frac{\partial \pi_t(x)}{\partial H_t(a)} = \frac{\partial}{\partial H_t(a)} \pi_t(x) = \frac{\partial}{\partial H_t(a)}\left[\frac{e^{H_t(x)}}{\sum_{y=1}^{k} e^{H_t(y)}}\right]$$

$$= \frac{\frac{\partial e^{H_t(x)}}{\partial H_t(a)} \sum_{y=1}^{k} e^{H_t(y)} - e^{H_t(x)} \frac{\partial \sum_{y=1}^{k} e^{H_t(y)}}{\partial H_t(a)}}{\left(\sum_{y=1}^{k} e^{H_t(y)}\right)^2}$$

$$= \frac{\mathbb{1}_{a=x}\, e^{H_t(x)} \sum_{y=1}^{k} e^{H_t(y)} - e^{H_t(x)} e^{H_t(a)}}{\left(\sum_{y=1}^{k} e^{H_t(y)}\right)^2}$$

$$= \frac{\mathbb{1}_{a=x}\, e^{H_t(x)}}{\sum_{y=1}^{k} e^{H_t(y)}} - \frac{e^{H_t(x)} e^{H_t(a)}}{\left(\sum_{y=1}^{k} e^{H_t(y)}\right)^2}$$

$$= \mathbb{1}_{a=x}\, \pi_t(x) - \pi_t(x)\pi_t(a)$$

$$\boxed{\frac{\partial \pi_t(x)}{\partial H_t(a)} = \pi_t(x)\left[\mathbb{1}_{a=x} - \pi_t(a)\right]}$$

---

## 2.9 Associative Search (Contextual Bandits) {#sec-ch02-2-9}

- Intermediate between the $k$-armed bandit problem & the full RL problem
- Like full RL: they learn a policy
- Like $k$-armed bandit: each action affects only the immediate reward
- Involves both trial & error learning to **search** for the best actions & **association** of these actions with the situations in which they are best
- **Full RL problem**: actions are allowed to affect the **next situation** as well as the reward.

---

## 2.10 Summary {#sec-ch02-2-10}

- Presented several simple ways of balancing exploration & exploitation:
  - **$\varepsilon$-greedy**: choose randomly a small fraction of the time
  - **UCB**: choose deterministically but achieve exploration by subtly favoring at each step the actions that have received fewer samples
  - **Gradient Bandit algorithms**: estimate action preferences, not action values, and favor the more preferred actions in a graded, probabilistic manner using a soft-max distribution
  - **Optimistic Initialization**: initializing estimates optimistically causes even greedy methods to explore significantly

- How do we know which of these methods is best?
  - We can run them all on a $k$ ($k=10$)-armed testbed and compare their performances, but they all have a parameter of their own.
  - To get a meaningful comparative study we have to consider their performance as a function of their parameter.
  - Simple learning curves would be too complex and crowded in a graph to make clear comparisons.
  - Instead we use a different kind of graph called **parameter study**:
    - We can estimate each point as the average reward obtained over 1000 steps with a particular algorithm at a particular parameter setting.
    - From the inverted U-shape of each algorithm's performance curve, we can deduce that the algorithms all perform best at an intermediate value of their parameter, neither too large nor too small.
    - We can study the algorithm's sensitivities to their respective parameters by studying/plotting their performance/average reward over a range of parameter values varying about an order of magnitude.

- These bandit methods studied in this chapter are considered state of the art despite their simplicity.

- **Gittins index** is a well-studied Bayesian approach to balancing exploration and exploitation in $k$-armed bandit problems.
  - **Posterior sampling** or **Thompson sampling** is used for action selection.

- In the Bayesian setting, it is generally not feasible to perform the outrageously large computation of an information state of a horizon, but efficient approximation could be done.
  - This turns the **bandit problem** to **full RL problem instance,** hence the need for approximate RL methods.
